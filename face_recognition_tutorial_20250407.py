# -*- coding: utf-8 -*-
"""face_recognition_tutorial_20250407

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1EUooZSr8zf5kdTgU7hcaWmd17osn4zGK

# 오늘 실습 새로 할거예요.
## colab 하나 만들어서 얼굴 이미지 업로드 해놓기
## 아래 코드 따라 해놓기
"""

# prompt: 파일 업로드 가장 짧은 예제

from google.colab import files
uploaded = files.upload()

!pip install -qq deepface
!pip install -qq ultralytics

import tensorflow_datasets as tfds
ds, ds_info = tfds.load("lfw", split="train", with_info=True)
print(ds_info)

!pip list | grep -E 'numpy|opencv-python|deepface|ultralytics'

!python -V

!nvidia-smi

from deepface import DeepFace

import numpy as np
import cv2
from deepface import DeepFace
from deepface.modules import preprocessing
from tqdm import tqdm

import matplotlib.pyplot as plt

import tensorflow as tf
import tensorflow_datasets as tfds

!ls -alh /root/.deepface

tf.__version__

gpus = tf.config.list_physical_devices('GPU')
print(gpus)

!ls

# Commented out IPython magic to ensure Python compatibility.
# %%time
# # detector
# detected_face = DeepFace.extract_faces('tk1.jpg', detector_backend="opencv")

print(type(detected_face))
print(len(detected_face))
print(type(detected_face[0]))
print(detected_face[0].keys())

print(detected_face[0]['face'].shape)
print(detected_face[0]['facial_area'])
print(detected_face[0]['confidence'])

img = cv2.imread('tk1.jpg')
img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)

for face in detected_face:
    x, y, w, h = face['facial_area']['x'], face['facial_area']['y'], face['facial_area']['w'], face['facial_area']['h']
    cv2.rectangle(img, (x, y), (x + w, y + h), (0, 255, 0), 2)

plt.imshow(img)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# # detector
# detected_face = DeepFace.extract_faces('tk1.jpg', detector_backend="mtcnn")

# Commented out IPython magic to ensure Python compatibility.
# %%time
# # detector
# 
# img = cv2.imread('tk1.jpg')
# 
# detected_face_yolo = DeepFace.extract_faces('tk1.jpg', detector_backend="yolov8")

img = cv2.imread('tk1.jpg')
img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)

for face in detected_face_yolo:
    x, y, w, h = face['facial_area']['x'], face['facial_area']['y'], face['facial_area']['w'], face['facial_area']['h']
    cv2.rectangle(img, (x, y), (x + w, y + h), (0, 255, 0), 2)

plt.imshow(img)

print(detected_face[0]['facial_area'])
print(detected_face_yolo[0]['facial_area'])

fig, axs = plt.subplots(1, 2, figsize=(10, 5))

axs[0].imshow(detected_face[0]['face'])
axs[1].imshow(detected_face_yolo[0]['face'])

plt.show()

preprocessed_face = preprocessing.resize_image(
    img=detected_face_yolo[0]['face'], target_size=(160, 160))
preprocessed_face = preprocessing.normalize_input(
    img=preprocessed_face, normalization='base')

plt.imshow(preprocessed_face[0])

# Commented out IPython magic to ensure Python compatibility.
# %%time
# embedding = DeepFace.represent(
#     preprocessed_face[0], model_name="Facenet",
#     enforce_detection=False, normalization='raw')

print(embedding[0].keys())
print(len(embedding[0]['embedding']))
print(np.array(embedding[0]['embedding']))

# Commented out IPython magic to ensure Python compatibility.
# %%time
# embedding1 = DeepFace.represent(
#     'tk1.jpg', model_name="Facenet", detector_backend="yolov8",
#     enforce_detection=True, normalization='base')

print(np.array(embedding1[0]['embedding']))

emb = np.array(embedding[0]['embedding'])  # yolo detector 를 사용해서 직접 뽑은 embedding
emb1 = np.array(embedding1[0]['embedding'])  # file 에서 deepface 자동으로 뽑아준 embedding

print(np.linalg.norm(emb))  # emb 의 벡터 크기 (L2 norm)
print(np.linalg.norm(emb1))

# L2 distance -> 거리
print(np.linalg.norm(emb - emb1))

# cos similarity -> 각도
emb_norm = emb / np.linalg.norm(emb)  # normalization: vector 의 크기를 1로 만듬
print(np.linalg.norm(emb_norm))  # = 1
emb1_norm = emb1 / np.linalg.norm(emb1)
print(np.dot(emb_norm, emb1_norm))

# tk1.jpg, tk2.jpg

detected_face2 = DeepFace.extract_faces('tk2.jpg', detector_backend="yolov8")

preprocessed_face2 = preprocessing.resize_image(
    img=detected_face2[0]['face'], target_size=(160, 160))
preprocessed_face2 = preprocessing.normalize_input(
    img=preprocessed_face2, normalization='base')

embedding2 = DeepFace.represent(
    preprocessed_face2[0], model_name="Facenet",
    enforce_detection=False, normalization='raw')

emb2 = np.array(embedding2[0]['embedding'])
emb2_norm = emb2 / np.linalg.norm(emb2)

print(emb_norm @ emb2_norm)
print(np.linalg.norm(emb - emb2))

img2 = cv2.imread('tk2.jpg')
img2 = cv2.cvtColor(img2, cv2.COLOR_BGR2RGB)

x, y, w, h = detected_face2[0]['facial_area']['x'], detected_face2[0]['facial_area']['y'], detected_face2[0]['facial_area']['w'], detected_face2[0]['facial_area']['h']
img_show2 = cv2.rectangle(img2, (x, y), (x + w, y + h), (0, 255, 0), 2)

plt.imshow(img_show2)

ds, ds_info = tfds.load("lfw", split="train", with_info=True)
print(ds_info)

all_images = []
all_names = []

for example in tqdm(tfds.as_numpy(ds)):
    all_images.append(example['image'])  # (250, 250, 3)
    all_names.append(example['label'].decode())  # text -> byte --> text

all_images = np.array(all_images)
all_names = np.array(all_names)

print(all_images.shape)  # 4D tensor
print(all_names.shape)

len(np.unique(all_names))  # 5749

rand_idx = np.random.randint(0, len(all_images))
print(rand_idx, all_names[rand_idx])
plt.imshow(all_images[rand_idx])

rand_idx = np.random.randint(0, len(all_images))

print(rand_idx)

image = all_images[rand_idx]
name = all_names[rand_idx]

embedding_lfw = DeepFace.represent(
    image, model_name="Facenet",
    detector_backend="yolov8",
    enforce_detection=True)

emb_lfw = np.array(embedding_lfw[0]['embedding'])
emb_lfw_norm = emb_lfw / np.linalg.norm(emb_lfw)

print(emb_norm @ emb_lfw_norm)
print(np.linalg.norm(emb - emb_lfw))

plt.imshow(image)
plt.title(f"Label: {name}")
plt.axis("off")
plt.show()

